{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Similitud entre Productos - V2 (Con Fine-Tuning)\n",
        "\n",
        "**Objetivo**: Determinar cuán similares son dos títulos de productos del dataset de test, generando un listado de pares ordenados por score de similitud.\n",
        "\n",
        "**Modelo Base**: SBERT (paraphrase-multilingual-mpnet-base-v2)\n",
        "\n",
        "**Fine-Tuning**: Entrenamiento con datos de `items_titles.csv` usando TSDAE (Transformer-based Sequential Denoising Auto-Encoder) para adaptación al dominio de productos.\n",
        "\n",
        "**Criterio de filtrado**: Score de similitud >= 0.9\n",
        "\n",
        "| Dataset | Productos | Uso |\n",
        "|---------|-----------|-----|\n",
        "| `items_titles.csv` | 30,000 | Fine-tuning del modelo |\n",
        "| `items_titles_test.csv` | 10,000 | Evaluación y generación de pares |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup completado\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "from utils_similarity import (\n",
        "    preprocess_dataframe,\n",
        "    preprocess_title,\n",
        "    analizar_longitud_titulos,\n",
        "    ProductSimilarity,\n",
        "    reduce_dimensions_3d,\n",
        "    cluster_embeddings,\n",
        "    plot_3d_clusters\n",
        ")\n",
        "\n",
        "# Configuración\n",
        "THRESHOLD = 0.9  # Filtrar pares con similitud >= 0.9\n",
        "FINETUNE_MODEL_PATH = './sbert_finetuned_productos'\n",
        "\n",
        "print(\"Setup completado\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Cargar Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset TRAIN (fine-tuning): 30,000 productos\n",
            "Dataset TEST: 10,000 productos\n",
            "\n",
            "Columnas train: ['ITE_ITEM_TITLE']\n",
            "Columnas test: ['ITE_ITEM_TITLE']\n"
          ]
        }
      ],
      "source": [
        "# Cargar dataset de entrenamiento (para fine-tuning)\n",
        "df_train = pd.read_csv('items_titles.csv')\n",
        "\n",
        "# Cargar dataset de test\n",
        "df_test = pd.read_csv('items_titles_test.csv')\n",
        "\n",
        "print(f\"Dataset TRAIN (fine-tuning): {len(df_train):,} productos\")\n",
        "print(f\"Dataset TEST: {len(df_test):,} productos\")\n",
        "print(f\"\\nColumnas train: {df_train.columns.tolist()}\")\n",
        "print(f\"Columnas test: {df_test.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplos de títulos (train):\n",
            "  1. Tênis Ascension Posh Masculino - Preto E Vermelho \n",
            "  2. Tenis Para Caminhada Super Levinho Spider Corrida \n",
            "  3. Tênis Feminino Le Parc Hocks Black/ice Original Envio Já\n",
            "  4. Tênis Olympikus Esportivo Academia Nova Tendência Triunfo \n",
            "  5. Inteligente Led Bicicleta Tauda Luz Usb Bicicleta Carregáve\n",
            "  6. Tênis Casual Masculino Zarato 941 Preto 632\n",
            "  7. Tênis Infantil Ortopasso Conforto Jogging\n",
            "  8. Tv Samsung Qled 8k Q800t Semi Nova\n",
            "  9. Tênis Usthemp Short Temático - Maria Vira-lata 2\n",
            "  10. Sapatênis West Coast Urban Couro Masculino\n"
          ]
        }
      ],
      "source": [
        "# Muestra de títulos de entrenamiento\n",
        "print(\"Ejemplos de títulos (train):\")\n",
        "for i, title in enumerate(df_train['ITE_ITEM_TITLE'].head(10)):\n",
        "    print(f\"  {i+1}. {title}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estadísticas de Longitud (Train):\n",
            "  Caracteres: media=47.0, mediana=50\n",
            "  Palabras:   media=7.2, mediana=7\n",
            "\n",
            "Estadísticas de Longitud (Test):\n",
            "  Caracteres: media=47.0, mediana=50\n",
            "  Palabras:   media=7.2, mediana=7\n"
          ]
        }
      ],
      "source": [
        "# Estadísticas de longitud de títulos (train)\n",
        "stats_train = analizar_longitud_titulos(df_train)\n",
        "stats_test = analizar_longitud_titulos(df_test)\n",
        "\n",
        "print(\"\\nEstadísticas de Longitud (Train):\")\n",
        "print(f\"  Caracteres: media={stats_train['char_length']['mean']:.1f}, mediana={stats_train['char_length']['median']:.0f}\")\n",
        "print(f\"  Palabras:   media={stats_train['word_count']['mean']:.1f}, mediana={stats_train['word_count']['median']:.0f}\")\n",
        "\n",
        "print(\"\\nEstadísticas de Longitud (Test):\")\n",
        "print(f\"  Caracteres: media={stats_test['char_length']['mean']:.1f}, mediana={stats_test['char_length']['median']:.0f}\")\n",
        "print(f\"  Palabras:   media={stats_test['word_count']['mean']:.1f}, mediana={stats_test['word_count']['median']:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ejemplos de preprocesamiento (train):\n",
            "\n",
            "  Original:    Tênis Ascension Posh Masculino - Preto E Vermelho \n",
            "  Procesado:   tênis ascension posh masculino preto e vermelho\n",
            "\n",
            "  Original:    Tenis Para Caminhada Super Levinho Spider Corrida \n",
            "  Procesado:   tenis para caminhada super levinho spider corrida\n",
            "\n",
            "  Original:    Tênis Feminino Le Parc Hocks Black/ice Original Envio Já\n",
            "  Procesado:   tênis feminino le parc hocks black ice original envio já\n",
            "\n",
            "  Original:    Tênis Olympikus Esportivo Academia Nova Tendência Triunfo \n",
            "  Procesado:   tênis olympikus esportivo academia nova tendência triunfo\n",
            "\n",
            "  Original:    Inteligente Led Bicicleta Tauda Luz Usb Bicicleta Carregáve\n",
            "  Procesado:   inteligente led bicicleta tauda luz usb bicicleta carregáve\n"
          ]
        }
      ],
      "source": [
        "# Preprocesar títulos de ambos datasets\n",
        "df_train = preprocess_dataframe(df_train, column='ITE_ITEM_TITLE', output_column='title_clean')\n",
        "df_test = preprocess_dataframe(df_test, column='ITE_ITEM_TITLE', output_column='title_clean')\n",
        "\n",
        "# Mostrar ejemplos antes/después\n",
        "print(\"Ejemplos de preprocesamiento (train):\")\n",
        "for i in range(5):\n",
        "    print(f\"\\n  Original:    {df_train['ITE_ITEM_TITLE'].iloc[i]}\")\n",
        "    print(f\"  Procesado:   {df_train['title_clean'].iloc[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Títulos para fine-tuning: 30,000\n",
            "Títulos para evaluación: 10,000\n"
          ]
        }
      ],
      "source": [
        "# Preparar listas\n",
        "titles_train_clean = df_train['title_clean'].tolist()\n",
        "titles_train_original = df_train['ITE_ITEM_TITLE'].tolist()\n",
        "\n",
        "titles_test_clean = df_test['title_clean'].tolist()\n",
        "titles_test_original = df_test['ITE_ITEM_TITLE'].tolist()\n",
        "\n",
        "print(f\"\\nTítulos para fine-tuning: {len(titles_train_clean):,}\")\n",
        "print(f\"Títulos para evaluación: {len(titles_test_clean):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Fine-Tuning del Modelo SBERT\n",
        "\n",
        "Usamos **TSDAE (Transformer-based Sequential Denoising Auto-Encoder)** para fine-tuning no supervisado.\n",
        "\n",
        "TSDAE funciona así:\n",
        "1. Corrompe los textos de entrada (eliminando/permutando palabras)\n",
        "2. El modelo aprende a reconstruir el texto original desde el embedding\n",
        "3. Esto adapta el modelo al dominio específico de productos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✗ No se encontró modelo fine-tuneado. Se procederá con el entrenamiento.\n"
          ]
        }
      ],
      "source": [
        "# Verificar si ya existe un modelo fine-tuneado\n",
        "if os.path.exists(FINETUNE_MODEL_PATH):\n",
        "    print(f\"✓ Modelo fine-tuneado encontrado en: {FINETUNE_MODEL_PATH}\")\n",
        "    SKIP_FINETUNING = True\n",
        "else:\n",
        "    print(f\"✗ No se encontró modelo fine-tuneado. Se procederá con el entrenamiento.\")\n",
        "    SKIP_FINETUNING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FINE-TUNING CON TSDAE\n",
            "============================================================\n",
            "\n",
            "1. Cargando modelo base...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "When tie_encoder_decoder=True, the decoder_name_or_path will be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Modelo: paraphrase-multilingual-mpnet-base-v2\n",
            "   Dimensión: 768\n",
            "\n",
            "2. Preparando dataset para TSDAE...\n",
            "   Samples: 30,000\n",
            "   Batches: 1,875\n",
            "\n",
            "3. Configurando TSDAE Loss...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForCausalLM were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. Iniciando entrenamiento...\n",
            "   Epochs: 1\n",
            "   Warmup steps: 100\n",
            "CPU times: user 3.29 s, sys: 978 ms, total: 4.27 s\n",
            "Wall time: 12.1 s\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Please install `datasets` to use this function: `pip install datasets`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mif not SKIP_FINETUNING:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    from sentence_transformers import SentenceTransformer, InputExample, losses\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    from sentence_transformers.datasets import DenoisingAutoEncoderDataset\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    from torch.utils.data import DataLoader\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*60)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFINE-TUNING CON TSDAE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*60)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # Cargar modelo base\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn1. Cargando modelo base...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    model_base = SentenceTransformer(\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mparaphrase-multilingual-mpnet-base-v2\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Modelo: paraphrase-multilingual-mpnet-base-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Dimensión: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mmodel_base.get_sentence_embedding_dimension()}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # Crear dataset para TSDAE\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn2. Preparando dataset para TSDAE...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    train_sentences = titles_train_clean\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # Dataset con noise (denoising)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    train_dataset = DenoisingAutoEncoderDataset(train_sentences)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # DataLoader\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    train_dataloader = DataLoader(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        train_dataset, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        batch_size=16, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        shuffle=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        drop_last=True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Samples: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mlen(train_sentences):,}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Batches: \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mlen(train_dataloader):,}\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # Loss function para TSDAE\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn3. Configurando TSDAE Loss...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    train_loss = losses.DenoisingAutoEncoderLoss(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        model_base, \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        decoder_name_or_path=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mparaphrase-multilingual-mpnet-base-v2\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        tie_encoder_decoder=True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # Entrenar\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn4. Iniciando entrenamiento...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Epochs: 1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m   Warmup steps: 100\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    model_base.fit(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        train_objectives=[(train_dataloader, train_loss)],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        epochs=1,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        warmup_steps=100,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        scheduler=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mwarmupcosine\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        optimizer_params=\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43mlr\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[33;43m: 3e-5},\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        show_progress_bar=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        use_amp=True  # Mixed precision para acelerar\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # Guardar modelo\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn5. Guardando modelo en: \u001b[39;49m\u001b[38;5;132;43;01m{FINETUNE_MODEL_PATH}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    model_base.save(FINETUNE_MODEL_PATH)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m + \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*60)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m✓ FINE-TUNING COMPLETADO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m=\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*60)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43melse:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    print(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSaltando fine-tuning (modelo ya existe)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/hazmat-science-2JPcahLu-py3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2571\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2572\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2576\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/hazmat-science-2JPcahLu-py3.11/lib/python3.11/site-packages/IPython/core/magics/execution.py:1447\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/hazmat-science-2JPcahLu-py3.11/lib/python3.11/site-packages/IPython/core/magics/execution.py:1411\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1409\u001b[39m st = clock2()\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:46\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/hazmat-science-2JPcahLu-py3.11/lib/python3.11/site-packages/sentence_transformers/fit_mixin.py:243\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03mDeprecated training method from before Sentence Transformers v3.0, it is recommended to use\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m:class:`~sentence_transformers.trainer.SentenceTransformerTrainer` instead. This method uses\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m \u001b[33;03m        store\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_datasets_available():\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install `datasets` to use this function: `pip install datasets`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# Delayed import to counter the SentenceTransformers -> FitMixin -> SentenceTransformerTrainer -> SentenceTransformers circular import\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n",
            "\u001b[31mImportError\u001b[39m: Please install `datasets` to use this function: `pip install datasets`."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "if not SKIP_FINETUNING:\n",
        "    from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "    from sentence_transformers.datasets import DenoisingAutoEncoderDataset\n",
        "    from torch.utils.data import DataLoader\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"FINE-TUNING CON TSDAE\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Cargar modelo base\n",
        "    print(\"\\n1. Cargando modelo base...\")\n",
        "    model_base = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "    print(f\"   Modelo: paraphrase-multilingual-mpnet-base-v2\")\n",
        "    print(f\"   Dimensión: {model_base.get_sentence_embedding_dimension()}\")\n",
        "    \n",
        "    # Crear dataset para TSDAE\n",
        "    print(\"\\n2. Preparando dataset para TSDAE...\")\n",
        "    train_sentences = titles_train_clean\n",
        "    \n",
        "    # Dataset con noise (denoising)\n",
        "    train_dataset = DenoisingAutoEncoderDataset(train_sentences)\n",
        "    \n",
        "    # DataLoader\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=16, \n",
        "        shuffle=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "    print(f\"   Samples: {len(train_sentences):,}\")\n",
        "    print(f\"   Batches: {len(train_dataloader):,}\")\n",
        "    \n",
        "    # Loss function para TSDAE\n",
        "    print(\"\\n3. Configurando TSDAE Loss...\")\n",
        "    train_loss = losses.DenoisingAutoEncoderLoss(\n",
        "        model_base, \n",
        "        decoder_name_or_path='paraphrase-multilingual-mpnet-base-v2',\n",
        "        tie_encoder_decoder=True\n",
        "    )\n",
        "    \n",
        "    # Entrenar\n",
        "    print(\"\\n4. Iniciando entrenamiento...\")\n",
        "    print(f\"   Epochs: 1\")\n",
        "    print(f\"   Warmup steps: 100\")\n",
        "    \n",
        "    model_base.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        epochs=1,\n",
        "        warmup_steps=100,\n",
        "        scheduler='warmupcosine',\n",
        "        optimizer_params={'lr': 3e-5},\n",
        "        show_progress_bar=True,\n",
        "        use_amp=True  # Mixed precision para acelerar\n",
        "    )\n",
        "    \n",
        "    # Guardar modelo\n",
        "    print(f\"\\n5. Guardando modelo en: {FINETUNE_MODEL_PATH}\")\n",
        "    model_base.save(FINETUNE_MODEL_PATH)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✓ FINE-TUNING COMPLETADO\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(\"Saltando fine-tuning (modelo ya existe)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerías y utilidades\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from utils_classifier import (\n",
        "    # Constantes\n",
        "    COST_FAILURE, COST_MAINTENANCE, ATTRIBUTES,\n",
        "    # Clases principales\n",
        "    FeatureEngineer, DataPreparator, ModelTrainer, OptunaOptimizer, Visualizer,\n",
        "    # Funciones\n",
        "    calculate_cost, calculate_baseline_cost, optimize_threshold,\n",
        "    save_model_artifacts, predict_failure_probability,\n",
        "    # Funciones de análisis\n",
        "    analyze_dataset, analyze_nulls, analyze_target_distribution, plot_cost_over_time,\n",
        "    plot_cost_over_time_with_model,\n",
        "    analyze_temporal, analyze_baseline_costs, analyze_split, analyze_smote,\n",
        "    analyze_threshold_optimization, analyze_final_model, print_executive_summary,\n",
        "    analyze_mean_comparison, analyze_correlation, analyze_device_failures,\n",
        "    select_best_boost_model, analyze_optimized_model_performance,\n",
        "    analyze_attribute_distributions, analyze_feature_engineering_results, train_baseline_model,\n",
        "    train_models_pipeline, run_optuna_optimization, run_threshold_optimization,\n",
        "    evaluate_final_model_performance\n",
        ")\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(\"✅ Librerías importadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Generación de Embeddings con Modelo Fine-Tuneado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Cargar modelo fine-tuneado\n",
        "print(f\"Cargando modelo fine-tuneado desde: {FINETUNE_MODEL_PATH}\")\n",
        "model_finetuned = SentenceTransformer(FINETUNE_MODEL_PATH)\n",
        "print(f\"Modelo cargado. Dimensión: {model_finetuned.get_sentence_embedding_dimension()}\")\n",
        "\n",
        "# Generar embeddings para el dataset de test\n",
        "print(f\"\\nGenerando embeddings para {len(titles_test_clean):,} productos...\")\n",
        "embeddings = model_finetuned.encode(\n",
        "    titles_test_clean,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "print(f\"\\nEmbeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Visualización 3D de Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducir dimensiones para visualización\n",
        "print(\"Reduciendo dimensiones a 3D...\")\n",
        "embeddings_3d = reduce_dimensions_3d(embeddings, method='pca')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clustering\n",
        "N_CLUSTERS = 5\n",
        "clusters = cluster_embeddings(embeddings, n_clusters=N_CLUSTERS)\n",
        "\n",
        "# Distribución de clusters\n",
        "print(\"\\nDistribución de productos por cluster:\")\n",
        "for i in range(N_CLUSTERS):\n",
        "    count = (clusters == i).sum()\n",
        "    print(f\"  Cluster {i}: {count:,} productos ({count/len(clusters)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización 3D\n",
        "fig = plot_3d_clusters(\n",
        "    embeddings_3d=embeddings_3d,\n",
        "    clusters=clusters,\n",
        "    titles=titles_test_clean,\n",
        "    title_plot='Clusters de Productos (Modelo Fine-Tuneado)'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Cálculo de Similitud y Filtrado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calcular matriz de similitud completa\n",
        "print(f\"Calculando matriz de similitud ({len(embeddings):,} x {len(embeddings):,})...\")\n",
        "sim_matrix = cosine_similarity(embeddings)\n",
        "print(f\"Matriz calculada. Shape: {sim_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraer pares del triángulo superior (evitar duplicados)\n",
        "n = len(titles_test_original)\n",
        "upper_tri_indices = np.triu_indices(n, k=1)\n",
        "scores = sim_matrix[upper_tri_indices]\n",
        "\n",
        "# Filtrar por threshold\n",
        "mask = scores >= THRESHOLD\n",
        "filtered_indices = np.where(mask)[0]\n",
        "\n",
        "print(f\"Total de pares posibles: {len(scores):,}\")\n",
        "print(f\"Pares con score >= {THRESHOLD}: {len(filtered_indices):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame con los pares filtrados\n",
        "results = []\n",
        "for idx in filtered_indices:\n",
        "    i = upper_tri_indices[0][idx]\n",
        "    j = upper_tri_indices[1][idx]\n",
        "    results.append({\n",
        "        'ITE_ITEM_TITLE': titles_test_original[i],\n",
        "        'ITE_ITEM_TITLE_2': titles_test_original[j],\n",
        "        'Score Similitud (0,1)': round(scores[idx], 4)\n",
        "    })\n",
        "\n",
        "df_output = pd.DataFrame(results)\n",
        "\n",
        "# Ordenar por score descendente\n",
        "df_output = df_output.sort_values('Score Similitud (0,1)', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nPares similares encontrados: {len(df_output):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vista previa de los pares más similares\n",
        "print(\"\\nTop 10 pares más similares:\")\n",
        "for i, row in df_output.head(10).iterrows():\n",
        "    print(f\"\\n[{row['Score Similitud (0,1)']:.4f}]\")\n",
        "    print(f\"  1: {row['ITE_ITEM_TITLE'][:70]}...\")\n",
        "    print(f\"  2: {row['ITE_ITEM_TITLE_2'][:70]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribución de scores\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.hist(df_output['Score Similitud (0,1)'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax.axvline(df_output['Score Similitud (0,1)'].mean(), color='red', linestyle='--', \n",
        "          label=f\"Media: {df_output['Score Similitud (0,1)'].mean():.4f}\")\n",
        "ax.set_xlabel('Score de Similitud')\n",
        "ax.set_ylabel('Frecuencia')\n",
        "ax.set_title(f'Distribución de Scores (threshold >= {THRESHOLD}) - Modelo Fine-Tuneado')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nEstadísticas de scores filtrados:\")\n",
        "print(df_output['Score Similitud (0,1)'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Comparación: Modelo Base vs Fine-Tuneado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Cargar modelo base (sin fine-tuning) para comparación\n",
        "print(\"Cargando modelo base para comparación...\")\n",
        "model_base = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "# Generar embeddings con modelo base\n",
        "print(f\"Generando embeddings con modelo BASE...\")\n",
        "embeddings_base = model_base.encode(\n",
        "    titles_test_clean,\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64\n",
        ")\n",
        "print(f\"Embeddings BASE shape: {embeddings_base.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular similitudes con modelo base\n",
        "sim_matrix_base = cosine_similarity(embeddings_base)\n",
        "scores_base = sim_matrix_base[upper_tri_indices]\n",
        "\n",
        "# Filtrar por threshold\n",
        "mask_base = scores_base >= THRESHOLD\n",
        "filtered_indices_base = np.where(mask_base)[0]\n",
        "\n",
        "print(f\"\\nComparación de resultados:\")\n",
        "print(f\"  Modelo BASE: {len(filtered_indices_base):,} pares con score >= {THRESHOLD}\")\n",
        "print(f\"  Modelo FINE-TUNED: {len(filtered_indices):,} pares con score >= {THRESHOLD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estadísticas comparativas\n",
        "print(\"\\nEstadísticas de Similitud:\")\n",
        "print(\"\\nModelo BASE:\")\n",
        "print(f\"  Media: {scores_base.mean():.4f}\")\n",
        "print(f\"  Std:   {scores_base.std():.4f}\")\n",
        "print(f\"  Min:   {scores_base.min():.4f}\")\n",
        "print(f\"  Max:   {scores_base.max():.4f}\")\n",
        "\n",
        "print(\"\\nModelo FINE-TUNED:\")\n",
        "print(f\"  Media: {scores.mean():.4f}\")\n",
        "print(f\"  Std:   {scores.std():.4f}\")\n",
        "print(f\"  Min:   {scores.min():.4f}\")\n",
        "print(f\"  Max:   {scores.max():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización comparativa\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histograma modelo base\n",
        "axes[0].hist(scores_base, bins=100, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "axes[0].axvline(scores_base.mean(), color='red', linestyle='--', label=f'Media: {scores_base.mean():.4f}')\n",
        "axes[0].axvline(THRESHOLD, color='green', linestyle=':', label=f'Threshold: {THRESHOLD}')\n",
        "axes[0].set_xlabel('Score de Similitud')\n",
        "axes[0].set_ylabel('Frecuencia')\n",
        "axes[0].set_title('Modelo BASE')\n",
        "axes[0].legend()\n",
        "\n",
        "# Histograma modelo fine-tuned\n",
        "axes[1].hist(scores, bins=100, alpha=0.7, color='coral', edgecolor='black')\n",
        "axes[1].axvline(scores.mean(), color='red', linestyle='--', label=f'Media: {scores.mean():.4f}')\n",
        "axes[1].axvline(THRESHOLD, color='green', linestyle=':', label=f'Threshold: {THRESHOLD}')\n",
        "axes[1].set_xlabel('Score de Similitud')\n",
        "axes[1].set_ylabel('Frecuencia')\n",
        "axes[1].set_title('Modelo FINE-TUNED')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle('Comparación: Distribución de Similitudes', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: correlación entre scores de ambos modelos\n",
        "# Tomar muestra para visualización\n",
        "sample_size = 10000\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.choice(len(scores), min(sample_size, len(scores)), replace=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.scatter(scores_base[sample_idx], scores[sample_idx], alpha=0.3, s=10)\n",
        "ax.plot([0, 1], [0, 1], 'r--', label='Referencia (x=y)')\n",
        "ax.axhline(THRESHOLD, color='green', linestyle=':', alpha=0.5, label=f'Threshold: {THRESHOLD}')\n",
        "ax.axvline(THRESHOLD, color='green', linestyle=':', alpha=0.5)\n",
        "ax.set_xlabel('Score Modelo BASE')\n",
        "ax.set_ylabel('Score Modelo FINE-TUNED')\n",
        "ax.set_title(f'Correlación de Scores\\n(muestra de {len(sample_idx):,} pares)')\n",
        "ax.legend()\n",
        "\n",
        "# Calcular correlación\n",
        "correlation = np.corrcoef(scores_base[sample_idx], scores[sample_idx])[0, 1]\n",
        "ax.text(0.05, 0.95, f'Correlación: {correlation:.4f}', transform=ax.transAxes, \n",
        "        fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Exportar CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar output final\n",
        "df_final = df_output.copy()\n",
        "df_final.columns = ['ITE_ITEM_TITLE', 'ITE_ITEM_TITLE_2', 'Score Similitud (0,1)']\n",
        "\n",
        "# Guardar\n",
        "OUTPUT_FILE = 'output_similitud_finetuned.csv'\n",
        "df_output.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"Archivo guardado: {OUTPUT_FILE}\")\n",
        "print(f\"Total de pares: {len(df_output):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# También guardar resultados del modelo base para comparación\n",
        "results_base = []\n",
        "for idx in filtered_indices_base:\n",
        "    i = upper_tri_indices[0][idx]\n",
        "    j = upper_tri_indices[1][idx]\n",
        "    results_base.append({\n",
        "        'ITE_ITEM_TITLE': titles_test_original[i],\n",
        "        'ITE_ITEM_TITLE_2': titles_test_original[j],\n",
        "        'Score Similitud (0,1)': round(scores_base[idx], 4)\n",
        "    })\n",
        "\n",
        "df_output_base = pd.DataFrame(results_base)\n",
        "df_output_base = df_output_base.sort_values('Score Similitud (0,1)', ascending=False).reset_index(drop=True)\n",
        "\n",
        "OUTPUT_FILE_BASE = 'output_similitud_base.csv'\n",
        "df_output_base.to_csv(OUTPUT_FILE_BASE, index=False)\n",
        "\n",
        "print(f\"\\nArchivo modelo base guardado: {OUTPUT_FILE_BASE}\")\n",
        "print(f\"Total de pares (base): {len(df_output_base):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Resumen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"RESUMEN - SIMILITUD ENTRE PRODUCTOS V2 (CON FINE-TUNING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nDatasets:\")\n",
        "print(f\"  - Train (fine-tuning): {len(df_train):,} productos\")\n",
        "print(f\"  - Test (evaluación): {len(df_test):,} productos\")\n",
        "\n",
        "print(f\"\\nModelo:\")\n",
        "print(f\"  - Base: paraphrase-multilingual-mpnet-base-v2\")\n",
        "print(f\"  - Fine-tuning: TSDAE (Denoising Auto-Encoder)\")\n",
        "print(f\"  - Dimensión embeddings: {embeddings.shape[1]}\")\n",
        "\n",
        "print(f\"\\nResultados Modelo FINE-TUNED (threshold >= {THRESHOLD}):\")\n",
        "print(f\"  - Pares encontrados: {len(df_output):,}\")\n",
        "print(f\"  - Score máximo: {df_output['Score Similitud (0,1)'].max():.4f}\")\n",
        "print(f\"  - Score mínimo: {df_output['Score Similitud (0,1)'].min():.4f}\")\n",
        "print(f\"  - Score medio: {df_output['Score Similitud (0,1)'].mean():.4f}\")\n",
        "\n",
        "print(f\"\\nResultados Modelo BASE (threshold >= {THRESHOLD}):\")\n",
        "print(f\"  - Pares encontrados: {len(df_output_base):,}\")\n",
        "print(f\"  - Score máximo: {df_output_base['Score Similitud (0,1)'].max():.4f}\")\n",
        "print(f\"  - Score mínimo: {df_output_base['Score Similitud (0,1)'].min():.4f}\")\n",
        "print(f\"  - Score medio: {df_output_base['Score Similitud (0,1)'].mean():.4f}\")\n",
        "\n",
        "print(f\"\\nArchivos de salida:\")\n",
        "print(f\"  - Fine-tuned: {OUTPUT_FILE}\")\n",
        "print(f\"  - Base: {OUTPUT_FILE_BASE}\")\n",
        "print(f\"  - Modelo: {FINETUNE_MODEL_PATH}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hazmat-science-2JPcahLu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
